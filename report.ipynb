{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ed4ec8e-dc9e-4f6a-a87a-527ee7f26184",
   "metadata": {},
   "source": [
    "# Nov 5th- ?\n",
    "\n",
    "- passed a list of found names to Marion to run through the name resolver, ~50% passed\n",
    "- Added name1 to ap_name1 as a \"name\" tag\n",
    "- llamaindex recursive retrieval, faced error but might be something of interest ...\n",
    "\n",
    "- \n",
    "- Rerun through model\n",
    "- How does the model work? better evaluation plots\n",
    "- Add tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0563039-899a-4da8-962b-2f0084fe99bb",
   "metadata": {},
   "source": [
    "# Nov 2nd- Nov 5th:\n",
    "\n",
    "[Report is here](https://api.wandb.ai/links/shoubaneh/yt4m6s6p) with plots of metrics of initial successful runs for finetuning a NER task with pretrained BERT model using PEFT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeefc5b-2810-4896-8534-c46acdf257dc",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------\n",
    "# Oct 30- Nov 2nd:\n",
    "\n",
    "## Done:\n",
    "- **Granuality**: Bionlp different from my data, changed to sentences so it can be handled with huggingface map and transformers fine tuning.\n",
    "- **GPU** on irsa works: First run of fine-tuning roberta from data loading to training with no error.\n",
    "- **W&B** works magically with transformers library, no setup needed\n",
    "- **PEFT** ([LoRA](https://arxiv.org/abs/2106.09685)) to train only subset of network parameters\n",
    "- **evaluation metric** a compute metric (to be improved) added for precision/recall/f1 ...\n",
    "\n",
    "\n",
    "## To do:\n",
    "- Major current downside, coordinates or vz1 are not currently linked with a source name\n",
    "- Fix number finding for coordinates\n",
    "- Decide if [RoBERTa](https://arxiv.org/abs/1907.11692)-large is the best choice model\n",
    "- Hyper parameter tuning, i.e., lr, num epochs, percentage of params to train, ...\n",
    "- **Inference using the model**\n",
    "     > **Note**: split pdf text do inference and combine with logic at end\n",
    "\n",
    "- Improve data/labels and run again\n",
    "- Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7719046d-1f2b-44c5-b8ac-dd3f76523495",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------\n",
    "# Oct 16-23 :\n",
    "\n",
    "## Done:\n",
    "- **Preprocess data**: Refer to notebook ['dataprep_NER.ipynb'](dataprep_NER.ipynb)\n",
    "- first pass through text and table --> location of \"label entitites\" in each saved in csv files\n",
    "- From CSV files, tokenize text and convert index to token number and save in CoNLL format (text only)\n",
    "- Combine all articles, and divide to train,test, validation\n",
    "- Initial pass of reading the NED dataset within the transformers pipeline for finetuning\n",
    "\n",
    "## To do:\n",
    "\n",
    "1. Table data to be added, figure out separate model or same (most probably separate)\n",
    "2. Improve data collection based on statistics\n",
    "3. redshift not proper now maybe look for word redshift or z followed by number in text instead\n",
    "4. plots of data\n",
    "5. Finalize the NER model and tokenizer to fine-tune\n",
    "6. Incorporate W&B\n",
    "7. Train on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0ac1f0-fac4-483c-bfdb-b53a4377b540",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------\n",
    "# Oct 9-16 :\n",
    "\n",
    "## Done:\n",
    "- **Extract tables from PDFs**: Refer to notebook [`pdftables.ipynb`](older/pdftables.ipynb). Utilizes computer vision with different APIs. Might be suitable for the proposal, even if it's not preferable for real-world use due to potential accuracy loss with HTMLs. \n",
    "- **Tables extracted with HTML (using beautiful soup)**: Requires minor processing (e.g., remove multi-index, clean non-byte-like characters) to be model-readable.\n",
    "- **Tried ChatGPT API in python**: Not free and seemed less accurate than the web application. The cause is unclear.\n",
    "- **Learning experience**: Gained knowledge on using LLMs within pipelines for QA tasks or table tasks.\n",
    "\n",
    "## To do:\n",
    "\n",
    "1. **Choose a LLM for NER (Named Entity Recognition) Task**:\n",
    "    - Text-based LLMs: BERT, RoBERTa, astroLlama, or options from NICK.\n",
    "    - Table-based LLMs: [google/tapas-large-finetuned-wtq](link_to_model)\n",
    "    \n",
    "    > **Note**: Human inspection before ingestion:\n",
    "    > - **Yes**: Emphasis on completeness over validity.\n",
    "    > - **No**: Emphasis on validity over completeness.\n",
    "\n",
    "2. **Fine-Tuning**:\n",
    "    - PEFT: Refer to [HuggingFace's PEFT blog](https://huggingface.co/blog/peft).\n",
    "    - Dataset considerations: size, test/train split, labeling, etc.\n",
    "    - GPU adequacy: Is one GPU sufficient?\n",
    "    - Duration: How long will the process take?\n",
    "\n",
    "3. **Utilize ChatGPT for Code Generation**.\n",
    "4. **Graphics**: Investigate validity, completeness, etc.\n",
    "5. **Additional Potential Tasks**: Are there any tasks NED might be interested in besides extraction? For instance, clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29407110-9e32-4dca-9814-a614804b452a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
