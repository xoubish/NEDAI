{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb6844-fe58-4dfb-a7ca-1c7433a60eae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import wandb\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080572ed-9a6d-4a52-972e-a14c8ca2d0e2",
   "metadata": {},
   "source": [
    "## 1. Load your dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebb7857-16fa-4d93-a5c6-bae39d884fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_sample(batch):\n",
    "    tokens_list = []\n",
    "    tags_list = []\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    \n",
    "    for line in batch['text']:\n",
    "        if line:  # non-empty line means we have a token-tag pair\n",
    "            token, tag = line.split()  # assuming space is the delimiter\n",
    "            tokens.append(token)\n",
    "            tags.append(tag)\n",
    "        else:  # empty line means end of sentence\n",
    "            tokens_list.append(tokens)\n",
    "            tags_list.append(tags)\n",
    "            tokens = []\n",
    "            tags = []\n",
    "    \n",
    "    # Add remaining tokens and tags if there's any\n",
    "    if tokens:\n",
    "        tokens_list.append(tokens)\n",
    "        tags_list.append(tags)\n",
    "    \n",
    "    return {'tokens': tokens_list, 'tags': tags_list}\n",
    "    \n",
    "\n",
    "data_files = {\n",
    "    'train': 'data/filtered_train.txt',\n",
    "    'validation': 'data/filtered_val.txt',\n",
    "    'test': 'data/filtered_test.txt'\n",
    "}\n",
    "\n",
    "# Load the dataset from local files without specifying a script\n",
    "dataset = load_dataset('text', data_files=data_files)\n",
    "pdataset = dataset.map(process_sample, batched=True, remove_columns=['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f089fbfc-c092-469d-bad8-e14b21ac27bb",
   "metadata": {},
   "source": [
    "## 2. Load the tokenizer and model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3795be-392b-44fc-89ab-6c65077d1744",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-ap_name1\",\n",
    "    2: \"I-ap_name1\",\n",
    "    3: \"B-vz1\",\n",
    "    4: \"I-vz1\",\n",
    "    5: \"B-coordx1\",\n",
    "    6: \"I-coordx1\",\n",
    "    7: \"B-coordy1\",\n",
    "    8: \"I-coordy1\",\n",
    "    9: \"B-type1\",\n",
    "    10: \"I-type1\",\n",
    "}\n",
    "label2id = {\"O\": 0,\n",
    "          \"B-ap_name1\": 1,\n",
    "          \"I-ap_name1\": 2,\n",
    "          \"B-vz1\": 3,\n",
    "          \"I-vz1\": 4,\n",
    "          \"B-coordx1\": 5,\n",
    "          \"I-coordx1\": 6,\n",
    "          \"B-coordy1\": 7,\n",
    "          \"I-coordy1\": 8,\n",
    "          \"B-type1\": 9,\n",
    "          \"I-type1\": 10,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e6eaf2-03f8-4430-8515-9a59d61ad4f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\", add_prefix_space=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\", num_labels=11, id2label=id2label, label2id=label2id,ignore_mismatched_sizes=True)\n",
    "\n",
    "# Define a data collator to handle token-level tasks (like NER)\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26296055-aea6-4ec1-8d17-60f71cf46cca",
   "metadata": {},
   "source": [
    "## 3. Tokenize the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac9aa3e-9b5c-4975-938b-2cd1663a283b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def recursive_label2id_conversion(label, label2id):\n",
    "    if isinstance(label, str):\n",
    "        return label2id[label]\n",
    "    elif isinstance(label, list):\n",
    "        return [recursive_label2id_conversion(l, label2id) for l in label]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported label type\")\n",
    "        \n",
    "def tokenize_and_align_labels2(examples, label2id):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"tags\"]):\n",
    "        converted_label = recursive_label2id_conversion(label, label2id)\n",
    "\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(converted_label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "#tokenized_datasets = pdataset.map(tokenize_function, batched=True, num_proc=4)\n",
    "tokenized_datap = pdataset.map(tokenize_and_align_labels2, batched=True, fn_kwargs={\"label2id\": label2id},num_proc=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4450100-f7f4-4a18-91cc-0cda80f70c58",
   "metadata": {},
   "source": [
    "## 4. Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb1b65-a1f5-4951-b11e-017635d2442c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.init(project='NEDAI',name='try1')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=30,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"wandb\",  # Log to wandb\n",
    "    logging_steps=20,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    output_dir=\"./results\",\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    data_collator = data_collator,\n",
    "    train_dataset = tokenized_datap[\"train\"],\n",
    "    eval_dataset = tokenized_datap[\"validation\"],\n",
    "    tokenizer = tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "wandb.finish()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./ner_model\")\n",
    "tokenizer.save_pretrained(\"./ner_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffad829e-ee8f-457f-9030-3e2e74af5458",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3931140c-147e-4cd6-89eb-cb8f9d4232e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./results/checkpoint-62000/\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"./results/checkpoint-62000/\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ef27365-910e-49eb-91e6-7ef2480168e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-ap_name1', 'score': 0.9999604, 'index': 12, 'word': 'M', 'start': 42, 'end': 43}\n",
      "{'entity': 'I-ap_name1', 'score': 0.99823296, 'index': 13, 'word': '##45', 'start': 43, 'end': 45}\n",
      "{'entity': 'B-ap_name1', 'score': 0.9999646, 'index': 39, 'word': 'S', 'start': 145, 'end': 146}\n",
      "{'entity': 'I-ap_name1', 'score': 0.9961766, 'index': 40, 'word': '##N', 'start': 146, 'end': 147}\n",
      "{'entity': 'I-ap_name1', 'score': 0.9987785, 'index': 41, 'word': '-', 'start': 147, 'end': 148}\n",
      "{'entity': 'I-ap_name1', 'score': 0.9999461, 'index': 42, 'word': '231', 'start': 148, 'end': 151}\n",
      "{'entity': 'I-ap_name1', 'score': 0.67516434, 'index': 43, 'word': '##8', 'start': 151, 'end': 152}\n"
     ]
    }
   ],
   "source": [
    "example = \"In this work we studied in detail galaxiy M45 which is at z=0.1 and very actively forming stars. We also compared this to a high redshift object SN-2318 which recently exploded nearby and might belong to a different host.\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "for n in ner_results:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4593e35-6e6d-47c3-bc7f-e350ca2cd9a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-ap_name1', 'score': 0.8148968, 'index': 47, 'word': 'M', 'start': 211, 'end': 212}\n",
      "{'entity': 'B-ap_name1', 'score': 0.99989116, 'index': 74, 'word': 'S', 'start': 314, 'end': 315}\n",
      "{'entity': 'I-ap_name1', 'score': 0.88695973, 'index': 75, 'word': '##N', 'start': 315, 'end': 316}\n",
      "{'entity': 'I-ap_name1', 'score': 0.99981934, 'index': 76, 'word': '-', 'start': 316, 'end': 317}\n",
      "{'entity': 'I-ap_name1', 'score': 0.9998771, 'index': 77, 'word': '231', 'start': 317, 'end': 320}\n"
     ]
    }
   ],
   "source": [
    "example = \"We study the high redshift universe and use the keck and hubble space telescope in our work.\\\n",
    "After careful analysis of data we came up with some interesting conclusions. In this work we studied in detail galaxiy M45 which is at z=0.1 and very actively forming stars. \\\n",
    "We also compared this to a high redshift object SN-2318 which recently exploded nearby and might belong to a different host.\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "for n in ner_results:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9880d3-0567-4fea-95b0-2a9cf927f496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
