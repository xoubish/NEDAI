{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d320fb7d-63ca-49ad-b58b-44a922669035",
   "metadata": {},
   "source": [
    "# Inference\n",
    "started Nov 3rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efd7bd4f-c91f-4ec3-bf59-33989f8e85b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-ap_name1\",\n",
    "    2: \"I-ap_name1\",\n",
    "    3: \"B-vz1\",\n",
    "    4: \"I-vz1\",\n",
    "    5: \"B-coordx1\",\n",
    "    6: \"I-coordx1\",\n",
    "    7: \"B-coordy1\",\n",
    "    8: \"I-coordy1\",\n",
    "    9: \"B-type1\",\n",
    "    10: \"I-type1\",\n",
    "}\n",
    "label2id = {\"O\": 0,\n",
    "          \"B-ap_name1\": 1,\n",
    "          \"I-ap_name1\": 2,\n",
    "          \"B-vz1\": 3,\n",
    "          \"I-vz1\": 4,\n",
    "          \"B-coordx1\": 5,\n",
    "          \"I-coordx1\": 6,\n",
    "          \"B-coordy1\": 7,\n",
    "          \"I-coordy1\": 8,\n",
    "          \"B-type1\": 9,\n",
    "          \"I-type1\": 10,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbcb3da3-b441-4018-985a-d7b4b0b3d581",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = \"roberta-large-lora-token-classification/checkpoint-108948/\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "inference_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    config.base_model_name_or_path, num_labels=11, id2label=id2label, label2id=label2id\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(inference_model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcd48744-85eb-449a-8bb2-a23f6a8040b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"In this work we studied in detail galaxies M45 and NGC 2132 which are at z=0.1 and they appeared to be red.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7defd423-77d2-49bf-a759-85771ad96b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s>', 'O')\n",
      "('In', 'O')\n",
      "('Ġthis', 'O')\n",
      "('Ġwork', 'O')\n",
      "('Ġwe', 'O')\n",
      "('Ġstudied', 'O')\n",
      "('Ġin', 'O')\n",
      "('Ġdetail', 'O')\n",
      "('Ġgalaxies', 'O')\n",
      "('ĠM', 'O')\n",
      "('45', 'O')\n",
      "('Ġand', 'O')\n",
      "('ĠN', 'O')\n",
      "('GC', 'O')\n",
      "('Ġ2', 'O')\n",
      "('132', 'O')\n",
      "('Ġwhich', 'O')\n",
      "('Ġare', 'O')\n",
      "('Ġat', 'O')\n",
      "('Ġz', 'O')\n",
      "('=', 'O')\n",
      "('0', 'O')\n",
      "('.', 'O')\n",
      "('1', 'O')\n",
      "('Ġand', 'O')\n",
      "('Ġthey', 'O')\n",
      "('Ġappeared', 'O')\n",
      "('Ġto', 'O')\n",
      "('Ġbe', 'O')\n",
      "('Ġred', 'O')\n",
      "('.', 'O')\n",
      "('</s>', 'O')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "tokens = inputs.tokens()\n",
    "predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "for token, prediction in zip(tokens, predictions[0].numpy()):\n",
    "    print((token, model.config.id2label[prediction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8345ac22-807f-490b-abe5-ca4825bb1ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
