{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70007b0c-45a8-4276-a8fc-1ed23bec83e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dslim/bert-base-NER and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([11]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([11, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The model 'PeftModelForTokenClassification' is not supported for ner. Supported models are ['AlbertForTokenClassification', 'BertForTokenClassification', 'BigBirdForTokenClassification', 'BioGptForTokenClassification', 'BloomForTokenClassification', 'CamembertForTokenClassification', 'CanineForTokenClassification', 'ConvBertForTokenClassification', 'Data2VecTextForTokenClassification', 'DebertaForTokenClassification', 'DebertaV2ForTokenClassification', 'DistilBertForTokenClassification', 'ElectraForTokenClassification', 'ErnieForTokenClassification', 'ErnieMForTokenClassification', 'EsmForTokenClassification', 'FlaubertForTokenClassification', 'FNetForTokenClassification', 'FunnelForTokenClassification', 'GPT2ForTokenClassification', 'GPT2ForTokenClassification', 'GPTBigCodeForTokenClassification', 'GPTNeoForTokenClassification', 'GPTNeoXForTokenClassification', 'IBertForTokenClassification', 'LayoutLMForTokenClassification', 'LayoutLMv2ForTokenClassification', 'LayoutLMv3ForTokenClassification', 'LiltForTokenClassification', 'LongformerForTokenClassification', 'LukeForTokenClassification', 'MarkupLMForTokenClassification', 'MegaForTokenClassification', 'MegatronBertForTokenClassification', 'MobileBertForTokenClassification', 'MPNetForTokenClassification', 'NezhaForTokenClassification', 'NystromformerForTokenClassification', 'QDQBertForTokenClassification', 'RemBertForTokenClassification', 'RobertaForTokenClassification', 'RobertaPreLayerNormForTokenClassification', 'RoCBertForTokenClassification', 'RoFormerForTokenClassification', 'SqueezeBertForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'XLMRobertaXLForTokenClassification', 'XLNetForTokenClassification', 'XmodForTokenClassification', 'YosoForTokenClassification'].\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "import evaluate\n",
    "\n",
    "\n",
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-name\",\n",
    "    2: \"I-name\",\n",
    "    3: \"B-redshift\",\n",
    "    4: \"I-redshift\",\n",
    "    5: \"B-RA\",\n",
    "    6: \"I-RA\",\n",
    "    7: \"B-DEC\",\n",
    "    8: \"I-DEC\",\n",
    "    9: \"B-Type\",\n",
    "    10: \"I-Type\",\n",
    "}\n",
    "label2id = {\"O\": 0,\n",
    "          \"B-name\": 1,\n",
    "          \"I-name\": 2,\n",
    "          \"B-redshift\": 3,\n",
    "          \"I-redshift\": 4,\n",
    "          \"B-RA\": 5,\n",
    "          \"I-RA\": 6,\n",
    "          \"B-DEC\": 7,\n",
    "          \"I-DEC\": 8,\n",
    "          \"B-Type\": 9,\n",
    "          \"I-Type\": 10,\n",
    "         }\n",
    "\n",
    "\n",
    "def get_html_text(f,plength=100):\n",
    "    with open(f) as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "        \n",
    "    #to inspect html and identify the class label\n",
    "    #print(soup.prettify()) \n",
    "    sections = soup.find_all('div', class_=\"article-text\")\n",
    "\n",
    "    # Extracting all paragraphs in the section\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text = ''\n",
    "    for i, para in enumerate(paragraphs):\n",
    "        p = para.get_text()\n",
    "        if (len(p)>plength) and (p[0].isalpha()):\n",
    "            text+=p\n",
    "            #print(f\"Paragraph {i+1}:\", p)\n",
    "            #print('--------------')\n",
    "    #text = re.sub(r'[^a-zA-Z0-9 .,]', '', text)#.lower()\n",
    "    return text\n",
    "    \n",
    "\n",
    "def format_pred_for_print(pred, paragraph, conf = 0.99):\n",
    "    '''\n",
    "    returns a pretty string with the predictions in paragraph highlighted.\n",
    "    pred: prediction output from a pipeline\n",
    "    paragraph: the original text the predictions were made on\n",
    "    '''\n",
    "    \n",
    "    RED_START = '\\x1b[31m'\n",
    "    RED_END = '\\x1b[0m'\n",
    "    \n",
    "    formatted_string=''\n",
    "    end=0\n",
    "    \n",
    "    for entry in pred:\n",
    "        if entry['score']>conf:\n",
    "            start = entry['start']\n",
    "            # add what's in between\n",
    "            formatted_string += paragraph[end:start]\n",
    "            # add the entry\n",
    "            end = entry['end']\n",
    "            label = entry['entity']\n",
    "            score = ' {:.2f}'.format(entry['score'])\n",
    "            formatted_string+= RED_START+'['+paragraph[start:end]+' ('+label+score+')]'+RED_END\n",
    "        \n",
    "    formatted_string+= paragraph[end:]\n",
    "    return(formatted_string)\n",
    "\n",
    "def extract_galaxy_names(sentence, predictions, confidence_level):\n",
    "    galaxy_names = []\n",
    "    current_name = \"\"\n",
    "    current_score = 0.0\n",
    "\n",
    "    for prediction in predictions:\n",
    "        entity = prediction['entity']\n",
    "        word = prediction['word']\n",
    "        score = prediction['score']\n",
    "\n",
    "        if entity == 'B-name':\n",
    "            # Check if the current name meets the confidence level and add it to the list\n",
    "            if current_name and current_score >= confidence_level:\n",
    "                galaxy_names.append(current_name)\n",
    "\n",
    "            # Start a new galaxy name and reset current score\n",
    "            current_name = word\n",
    "            current_score = score\n",
    "\n",
    "        elif entity == 'I-name' and current_name:\n",
    "            # Continue building the current galaxy name\n",
    "            current_name += word\n",
    "\n",
    "    # Add the last found name if it meets the confidence level\n",
    "    if current_name and current_score >= confidence_level:\n",
    "        galaxy_names.append(current_name)\n",
    "\n",
    "    return galaxy_names\n",
    "\n",
    "def names_in_paper(htmlfilepath):\n",
    "    texts = get_html_text(htmlfilepath)\n",
    "    sentences = sent_tokenize(texts)\n",
    "    galaxy_names = []\n",
    "    for s in sentences:\n",
    "        pred = nlpeft(s)\n",
    "        ex = extract_galaxy_names(s, pred,confidence_level=0.99)\n",
    "        for e in ex:\n",
    "            galname = re.sub(r'[^a-zA-Z0-9]', '', e)\n",
    "            if len(galname)>1:\n",
    "                galaxy_names.append(galname)\n",
    "    return list(set(galaxy_names))\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "peft_model_id = 'NER-BERT-lora-token-classification/nov22//'\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "tokenizerpeft = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "inference_model = AutoModelForTokenClassification.from_pretrained(config.base_model_name_or_path, num_labels=11, id2label=id2label, label2id=label2id,ignore_mismatched_sizes=True)\n",
    "modelpeft = PeftModel.from_pretrained(inference_model, peft_model_id)\n",
    "nlpeft = pipeline(\"ner\", model=modelpeft, tokenizer=tokenizerpeft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b24a020-7fb9-4256-8607-c0ee54955f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CrabNebula',\n",
       " 'FRB201211',\n",
       " 'FRB20180916B',\n",
       " 'NGC13131',\n",
       " 'FRB20190520B',\n",
       " 'FRB20200120',\n",
       " 'AXIS',\n",
       " 'SS433',\n",
       " 'NGC7793',\n",
       " 'NGC779',\n",
       " 'FRB20121102',\n",
       " 'NGC5408X',\n",
       " 'M51']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_in_paper('data/htmls/2022ApJ...937....5S.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e5b812c-5f58-4b95-946d-cc8ea0e3d536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FRB201211', 'FRB20190520', 'UX1', 'NGC1313X', 'NGC779', 'NGC5408X', 'M51']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_in_paper('data/htmls/2022ApJ...937....5S.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86b1770-7a2d-4a4e-810d-8372747b7ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = 'data/lable_locations2/'\n",
    "\n",
    "count = 0\n",
    "with open('listofnames.txt', 'w') as file:\n",
    "\n",
    "    for prepfilename in tqdm(os.listdir(outdir)):\n",
    "        if count>200:\n",
    "            break\n",
    "        prepfilepath = os.path.join(outdir, prepfilename)\n",
    "        if os.path.isfile(prepfilepath):\n",
    "            # Read in html sections and tables\n",
    "            start_time = time.time()  # Time at the start of the iteration\n",
    "\n",
    "            s = prepfilename.split('.')\n",
    "            htmldir = 'data/'+s[0][0:4]+'-'+s[0][4:]+'-Vol'+s[3][0:3]+'/HTML/'            \n",
    "            htmlfilepath = os.path.join(htmldir, prepfilename[0:19]+'.html')\n",
    "            unique_galnames = names_in_paper(htmlfilepath)\n",
    "    \n",
    "            end_time = time.time()  # Time at the end of the iteration\n",
    "            print(end_time-start_time,'seconds',unique_galnames)\n",
    "            for wor in unique_galnames:\n",
    "                file.write(wor + '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b1cb0-e1a9-4bb7-b13d-7947b5aebc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepfilename = '2022ApJ...936...10M_A10M.flt'\n",
    "s = prepfilename.split('.')\n",
    "htmldir = 'data/'+s[0][0:4]+'-'+s[0][4:]+'-Vol'+s[3][0:3]+'/HTML/'\n",
    "htmlfilepath = os.path.join(htmldir, prepfilename[0:19]+'.html')\n",
    "texts = get_html_text(htmlfilepath)\n",
    "sentences = sent_tokenize(texts)\n",
    "for s in sentences[30:350]:\n",
    "    pred = nlpeft(s)\n",
    "    print(format_pred_for_print(pred,s,conf=0.99))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77a9bac2-76c1-44a7-a210-0b019f64067b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/shemmati/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a563be-341a-45de-8bda-1bf28e91e061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
