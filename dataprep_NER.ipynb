{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "405ffb67-430e-4d98-8670-a0ec5453358d",
   "metadata": {},
   "source": [
    "# Preparing Data for Fine-tuning a NER Model\n",
    "started Oct 17th, last edit: Oct 19th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f56069-da58-4756-81bf-59f021663f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a679815f-89fb-49e6-8713-e49e4d929a15",
   "metadata": {},
   "source": [
    "## 1. **Data/Label Collection**\n",
    "- Reading in htmls, extracting text and table from it\n",
    "- Reading in labels extracted by NED figuring out where in the text/table they come from\n",
    "- Add the location in text/table as an extra column to the label file (to be used for annotation).\n",
    "- If labeled dataset is insufficient, we should consider augmenting it with more representative examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03ed9ac1-98ab-462e-80b9-26d71ccfbde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(s):\n",
    "    \"\"\"Clean the string by retaining only alphabetical and numerical values and converting to lowercase.\"\"\"\n",
    "    return re.sub(r'[^a-zA-Z0-9]', '', s).lower()\n",
    "\n",
    "def generate_variants(s):\n",
    "    \"\"\"Generate various forms of the string.\"\"\"\n",
    "    s = str(s).strip()\n",
    "    cleaned = clean_string(s)\n",
    "    with_dash = s.replace(' ', '-')\n",
    "    with_mdash = s.replace(' ', '_')\n",
    "    without_mdash = s.replace('-',' ')\n",
    "    withou_dash = s.replace('_', ' ')\n",
    "    nospace = s.replace(' ','')\n",
    "    variants = [s, cleaned, with_dash, with_mdash,nospace,withou_dash,without_mdash]\n",
    "    return variants\n",
    "\n",
    "def cleantable(df):\n",
    "    '''change multi-index column to single and \n",
    "    Iterate through each row and column in the DataFrame to remove non byte like characters'''\n",
    "    \n",
    "    df.columns = [' '.join(col).strip() for col in df.columns.values]\n",
    "    for index, row in df.iterrows():\n",
    "        for col in df.columns:\n",
    "            df.at[index, col] = re.sub('[^a-zA-Z0-9]', '', str(row[col]))\n",
    "    return df\n",
    "\n",
    "def clean_tables_list(tables_list):\n",
    "    '''Iterate over a list of tables and apply cleantable function to each one'''\n",
    "    return [cleantable(df) for df in tables_list]\n",
    "\n",
    "def get_html_tables(f):\n",
    "    with open(f) as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "    try:\n",
    "        tables = pd.read_html(str(soup))\n",
    "        clean_tables_list(tables)\n",
    "        return tables\n",
    "    except:\n",
    "        #print('no table in'+str(f))\n",
    "        pass\n",
    "    \n",
    "def get_html_text(f,plength=100):\n",
    "    with open(f) as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "        \n",
    "    #to inspect html and identify the class label\n",
    "    #print(soup.prettify()) \n",
    "    \n",
    "    sections = soup.find_all('div', class_=\"article-text\")\n",
    "\n",
    "    # Extracting all paragraphs in the section\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text = ''\n",
    "    for i, para in enumerate(paragraphs):\n",
    "        p = para.get_text()\n",
    "        if (len(p)>plength) and (p[0].isalpha()):\n",
    "            text+=p\n",
    "            #print(f\"Paragraph {i+1}:\", p)\n",
    "            #print('--------------')\n",
    "    return text\n",
    "\n",
    "def find_all(text, substring):\n",
    "    return [match.start() for match in re.finditer(substring, text, re.IGNORECASE)]\n",
    "\n",
    "def find_variant_in_text(sub, text):\n",
    "    \"\"\"Find the start and end index of a variant of 'sub' in 'text'.\"\"\"\n",
    "    ls =[]\n",
    "    for variant in generate_variants(sub):\n",
    "        start_index = find_all(text,variant)\n",
    "        if start_index:  # This condition checks if the list is not empty\n",
    "            for s in start_index:\n",
    "                ls.append((s, s + len(variant)))\n",
    "    return ls if ls else None\n",
    "\n",
    "def find_variant_in_tables(sub, tables_list):\n",
    "    \"\"\"Find all table numbers, row indices, and column names where a variant of 'sub' exists.\"\"\"\n",
    "    locations = []\n",
    "    \n",
    "    for variant in generate_variants(str(sub)):\n",
    "        for table_num, table in enumerate(tables_list, 1):\n",
    "            locs = table.isin([variant]).stack()\n",
    "            matched_locs = locs[locs == True].index.tolist()\n",
    "            for row, col in matched_locs:\n",
    "                locations.append((table_num, row, col))\n",
    "                \n",
    "    return locations if locations else None\n",
    "    \n",
    "def add_location_columns(df, text,tables_list,colname='ap_name1'):\n",
    "    \"\"\"Add 'Location' column to DataFrame based on the 'ap_name1' column values' appearance in the text.\"\"\"\n",
    "    if colname in df.columns:\n",
    "        df[colname+'_Loc_in_text'] = df[colname].apply(lambda cell: find_variant_in_text(cell, text) if pd.notna(cell) else None)\n",
    "\n",
    "        if tables_list:\n",
    "            df[colname+'_Loc_in_table'] = df[colname].apply(lambda cell: find_variant_in_tables(cell, tables_list) if pd.notna(cell) else None)\n",
    "        else:\n",
    "            df[colname+'_Loc_in_table'] = None\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dda383-ea74-4c51-8328-ebdfc7e26493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████▉                                   | 135/931 [02:21<06:48,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---in file:2022ApJ...925L..20K_L20K.flt---\n",
      "An error occurred: [Errno 2] No such file or directory: 'data/2022-ApJ-Vol925/HTML/2022ApJ...925L..20K.html'\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|██████████████                           | 318/931 [12:31<07:37,  1.34it/s]"
     ]
    }
   ],
   "source": [
    "directory = 'data/2022ApJ_PREPFILES/'\n",
    "outdir = 'data/lable_locations/'\n",
    "\n",
    "count = 0\n",
    "for prepfilename in tqdm(os.listdir(directory)):\n",
    "    if count>1000:\n",
    "        break\n",
    "    prepfilepath = os.path.join(directory, prepfilename)\n",
    "    if os.path.isfile(prepfilepath):\n",
    "        \n",
    "        # Read in label file \n",
    "        labeldf = pd.read_csv(prepfilepath, delimiter=\"|\", skipinitialspace=True, low_memory=False)\n",
    "        labeldf.columns = labeldf.columns.str.strip()\n",
    "        \n",
    "\n",
    "        try:\n",
    "            # Read in html sections and tables\n",
    "            s = prepfilename.split('.')\n",
    "            htmldir = 'data/'+s[0][0:4]+'-'+s[0][4:]+'-Vol'+s[3][0:3]+'/HTML/'\n",
    "            htmlfilepath = os.path.join(htmldir, prepfilename[0:19]+'.html')\n",
    "            texts = get_html_text(htmlfilepath)\n",
    "            tables = get_html_tables(htmlfilepath)\n",
    "            \n",
    "            #find where in section texts or table the labeled data are mentioned\n",
    "            df = add_location_columns(labeldf,texts,tables,colname='ap_name1')\n",
    "            df = add_location_columns(labeldf,texts,tables,colname='vz1')\n",
    "            df = add_location_columns(labeldf,texts,tables,colname='coordx1')\n",
    "            df = add_location_columns(labeldf,texts,tables,colname='coordy1')\n",
    "            \n",
    "            df.to_csv(outdir+prepfilename[:-3]+'csv', index=False)\n",
    "            count+=1\n",
    "        except Exception as e:\n",
    "            print('---in file:'+prepfilename+'---')\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            print('---------------------------')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cee01be-5263-499e-adf1-777f074babf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def count_entries_for_columns(directory, columns):\n",
    "    \"\"\"\n",
    "    Count the number of non-null entries in each column across all CSV files in the directory.\n",
    "    \"\"\"\n",
    "    counts = {col: 0 for col in columns}\n",
    "\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(directory, file))\n",
    "        for col in columns:\n",
    "            if col in df.columns:\n",
    "                counts[col] += df[col].count()  # Count non-null entries in the column\n",
    "    \n",
    "    return counts\n",
    "\n",
    "# Columns you are interested in\n",
    "columns = ['ap_name1', 'vz1','coordx1'] \n",
    "columnstext = ['ap_name1_Loc_in_text', 'vz1_Loc_in_text','coordx1_Loc_in_text'] \n",
    "columnstables = ['ap_name1_Loc_in_table', 'vz1_Loc_in_table', 'coordx1_Loc_in_table'] \n",
    "\n",
    "# Count files containing each column\n",
    "counts1 = count_entries_for_columns('data/lable_locations/', columns)\n",
    "counts2 = count_entries_for_columns('data/lable_locations/', columnstext)\n",
    "counts3 = count_entries_for_columns('data/lable_locations/', columnstables)\n",
    "\n",
    "counts_list = [counts1,counts2,counts3]\n",
    "l = ['all','found_intext','found_intable']\n",
    "width = 0.3\n",
    "for idx, counts in enumerate(counts_list):\n",
    "    print(counts)\n",
    "    plt.bar([i + idx*width for i in range(3)], counts.values(), width=width, label=l[idx])\n",
    "\n",
    "\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Number of Entries')\n",
    "plt.title('Number of Entries for Each Column')\n",
    "plt.xticks([i + width for i in range(len(columns))], columns, rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8d3978-d30e-482b-baf6-a92438704fbd",
   "metadata": {},
   "source": [
    "### some statistics on the data at hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b9d6d8-1e9d-4c53-96aa-6aab8ac00a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def count_files_with_columns(directory, columns):\n",
    "    \"\"\"\n",
    "    Count the number of files in the directory containing each column in the columns list.\n",
    "    \"\"\"\n",
    "    counts = {col: 0 for col in columns}\n",
    "\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "\n",
    "    for file in files:\n",
    "        # Check if the file is a CSV\n",
    "        if not file.endswith('.csv'):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(os.path.join(directory, file))\n",
    "            for col in columns:\n",
    "                if col in df.columns:\n",
    "                    counts[col] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file}: {e}\")\n",
    "    \n",
    "    return counts\n",
    "\n",
    "\n",
    "def plot_pie_chart(counts):\n",
    "    \"\"\"\n",
    "    Plot a pie chart based on the counts dictionary.\n",
    "    \"\"\"\n",
    "    labels = counts.keys()\n",
    "    sizes = counts.values()\n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    plt.title(\"Percentage of files with specific column names\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Columns you are interested in\n",
    "columns = ['ap_name1', 'ap_name1_Loc_in_text','ap_name1_Loc_in_table']  # Replace with your column names\n",
    "\n",
    "# Count files containing each column\n",
    "counts = count_files_with_columns('data/lable_locations/', columns)\n",
    "\n",
    "\n",
    "# Plot pie chart\n",
    "plot_pie_chart(counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320d2c15-b4c9-4fc3-a1af-02f9e831e5ba",
   "metadata": {},
   "source": [
    "## 2. **Annotation**\n",
    "- Mark and label entities within your text.\n",
    "- Entities to start with: `Object Name`, `RA`, `DEC`, `Redshift`, `Type`. We may add more later.\n",
    "#### 2.1 **Figure out annotation formats**\n",
    "Data can be represented in various formats:\n",
    "- **BIO (or IOB) Format**\n",
    "- **CoNLL Format**: Columns-based, used in datasets like CoNLL-2003. **will go with this for now**\n",
    "- **Spacy Format**: JSON format (for Spacy users) with entities represented by start/end character positions.\n",
    "\n",
    "Manual annotation can be time-consuming. If NED had not already done some part of this we could have considered: [Doccano](https://doccano.herokuapp.com/), [Prodigy](https://prodi.gy/) (by Spacy creators, paid), [Labelbox](https://www.labelbox.com/), or [Brat](http://brat.nlplab.org/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b2bab6-e700-4ac7-9362-0b6f14f1534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"John S. Maro lives in New York and is tired. He is 27.2 years old\"\n",
    "entities = [(\"John S. Maro\", \"PERSON\"), (\"New York\", \"LOCATION\"), (\"27.2\",\"AGE\")]\n",
    "\n",
    "# Step 1: Tokenize\n",
    "tokens = text.split()  # Simplistic whitespace tokenization\n",
    "labels = ['O'] * len(tokens)  # Step 2: Initialize with 'O' tags\n",
    "\n",
    "# Step 3: Match entities and assign tags\n",
    "for entity, entity_type in entities:\n",
    "    entity_tokens = entity.split()\n",
    "    for i in range(len(tokens) - len(entity_tokens) + 1):\n",
    "        if tokens[i:i+len(entity_tokens)] == entity_tokens:\n",
    "            labels[i] = \"B-\" + entity_type\n",
    "            for j in range(1, len(entity_tokens)):\n",
    "                labels[i+j] = \"I-\" + entity_type\n",
    "\n",
    "# Step 4: Compile to CoNLL format\n",
    "conll_data = \"\\n\".join([f\"{token} {label}\" for token, label in zip(tokens, labels)])\n",
    "\n",
    "print(conll_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbef0a9-9890-4789-bde3-a02649b8748e",
   "metadata": {},
   "source": [
    "### 3. **Train/Test Split**\n",
    "- Consider an 80% training, 10% validation, and 10% test split.\n",
    "- Respect document boundaries to avoid overlap between sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ff4a8-69a3-49e1-a304-0bb4b0b6a729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03e117e8-49f2-47dc-91d8-b4b8466f1cf0",
   "metadata": {},
   "source": [
    "### 4. **Preprocessing**\n",
    "- Tokenize consistently with the pre-trained model's tokenization.\n",
    "- Other steps might include converting to lowercase, handling punctuation, etc.\n",
    "\n",
    "### 5. **Model-Specific Formatting**\n",
    "- Convert data to be compatible with your chosen framework.\n",
    "- For HuggingFace Transformers, use their `TokenClassification` model format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33293be4-3580-4197-b5d6-3991fe7b584c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "add9b995-d2d0-4c0a-ba8d-d507c7244417",
   "metadata": {},
   "source": [
    "### 6. **Augmentation (Optional)**\n",
    "For smaller datasets, consider:\n",
    "- Back translation\n",
    "- Synonym replacement\n",
    "- Sentence shuffling\n",
    "\n",
    "### 7. **Data Quality Checks**\n",
    "- Ensure annotation consistency.\n",
    "- Address issues like overlapping annotations or mislabeled entities.\n",
    "\n",
    "After data preparation, proceed with fine-tuning your NER model, evaluating on the validation set and tuning hyperparameters as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ebed2-ee91-4d28-9e0e-09279fce6a11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
