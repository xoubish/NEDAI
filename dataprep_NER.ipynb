{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "405ffb67-430e-4d98-8670-a0ec5453358d",
   "metadata": {},
   "source": [
    "# Preparing Data for Fine-tuning a NER Model\n",
    "started Oct 17th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f56069-da58-4756-81bf-59f021663f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a679815f-89fb-49e6-8713-e49e4d929a15",
   "metadata": {},
   "source": [
    "### 1. **Data/Label Collection**\n",
    "- Reading in htmls, extracting text and table from it\n",
    "- Reading in labels extracted by NED figuring out where in the text/table they come from\n",
    "- Add the location in text/table as an extra column to the label file (to be used for annotation).\n",
    "- If labeled dataset is insufficient, we should consider augmenting it with more representative examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03ed9ac1-98ab-462e-80b9-26d71ccfbde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(s):\n",
    "    \"\"\"Clean the string by retaining only alphabetical and numerical values and converting to lowercase.\"\"\"\n",
    "    return re.sub(r'[^a-zA-Z0-9]', '', s).lower()\n",
    "\n",
    "def generate_variants(s):\n",
    "    \"\"\"Generate various forms of the string.\"\"\"\n",
    "    cleaned = clean_string(s)\n",
    "    with_dash = s.replace(' ', '-')\n",
    "    with_mdash = s.replace(' ', 'â€”')\n",
    "    variants = [s, cleaned]#, with_dash, with_mdash]\n",
    "    return variants\n",
    "\n",
    "def cleantable(df):\n",
    "    '''change multi-index column to single and \n",
    "    Iterate through each row and column in the DataFrame to remove non byte like characters'''\n",
    "    \n",
    "    df.columns = [' '.join(col).strip() for col in df.columns.values]\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        for col in df.columns:\n",
    "            df.at[index, col] = re.sub('[^a-zA-Z0-9]', '', str(row[col]))\n",
    "    return df\n",
    "\n",
    "def clean_tables_list(tables_list):\n",
    "    '''Iterate over a list of tables and apply cleantable function to each one'''\n",
    "    return [cleantable(df) for df in tables_list]\n",
    "\n",
    "def get_html_tables(f):\n",
    "    with open(f) as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "    tables = pd.read_html(str(soup))\n",
    "    clean_tables_list(tables)\n",
    "    return tables\n",
    "    \n",
    "def get_html_text(f,plength=100):\n",
    "    with open(f) as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "        \n",
    "    #to inspect html and identify the class label\n",
    "    #print(soup.prettify()) \n",
    "    \n",
    "    sections = soup.find_all('div', class_=\"article-text\")\n",
    "\n",
    "    # Extracting all paragraphs in the section\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text = ''\n",
    "    for i, para in enumerate(paragraphs):\n",
    "        p = para.get_text()\n",
    "        if (len(p)>plength) and (p[0].isalpha()):\n",
    "            text+=p\n",
    "            #print(f\"Paragraph {i+1}:\", p)\n",
    "            #print('--------------')\n",
    "    return text\n",
    "    \n",
    "def find_variant_in_text(sub, text):\n",
    "    \"\"\"Find the start and end index of a variant of 'sub' in 'text'.\"\"\"\n",
    "    for variant in generate_variants(str(sub)):\n",
    "        start_index = text.find(variant)\n",
    "        if start_index != -1:\n",
    "            return (start_index, start_index + len(variant))\n",
    "    return None\n",
    "\n",
    "def find_variant_in_tables(sub, tables_list):\n",
    "    \"\"\"Find all table numbers, row indices, and column names where a variant of 'sub' exists.\"\"\"\n",
    "    locations = []\n",
    "    \n",
    "    for variant in generate_variants(str(sub)):\n",
    "        for table_num, table in enumerate(tables_list, 1):\n",
    "            locs = table.isin([variant]).stack()\n",
    "            matched_locs = locs[locs == True].index.tolist()\n",
    "            for row, col in matched_locs:\n",
    "                locations.append((table_num, row, col))\n",
    "                \n",
    "    return locations if locations else None\n",
    "    \n",
    "def add_location_columns(df, text,tables_list,colname='ap_name1'):\n",
    "    \"\"\"Add 'Location' column to DataFrame based on the 'ap_name1' column values' appearance in the text.\"\"\"\n",
    "    df[colname+'_Loc_in_text'] = df[colname].apply(lambda cell: find_variant_in_text(cell, text) if pd.notna(cell) else None)\n",
    "    df[colname+'_Loc_in_table'] = df[colname].apply(lambda cell: find_variant_in_tables(cell, tables_list) if pd.notna(cell) else None)\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fa8c930-e30a-4642-90b4-a35fbe1263ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in html sections and tables\n",
    "f = '2022ApJ...924...14P.html'\n",
    "texts = get_html_text(f)\n",
    "tables = get_html_tables(f)\n",
    "\n",
    "# Read in label file \n",
    "labeldf = pd.read_csv('/Users/shemmati/Desktop/2022ApJ_PREPFILES/2022ApJ...924...14P_A14P.flt', delimiter=\"|\", skipinitialspace=True)\n",
    "labeldf.columns = labeldf.columns.str.strip()\n",
    "\n",
    "#find where in section or table they are mentioned\n",
    "ddf = add_location_columns(labeldf,texts,tables,colname='ap_name1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320d2c15-b4c9-4fc3-a1af-02f9e831e5ba",
   "metadata": {},
   "source": [
    "### 2. **Annotation**\n",
    "- Mark and label entities within your text.\n",
    "- Entities to start with: `Object Name`, `RA`, `DEC`, `Redshift`, `Type`. We may add more later.\n",
    "#### 2.1 **Figure out annotation formats**\n",
    "Data can be represented in various formats:\n",
    "- **BIO (or IOB) Format**\n",
    "- **CoNLL Format**: Columns-based, used in datasets like CoNLL-2003. **will go with this for now**\n",
    "- **Spacy Format**: JSON format (for Spacy users) with entities represented by start/end character positions.\n",
    "\n",
    "Manual annotation can be time-consuming. If NED had not already done some part of this we could have considered: [Doccano](https://doccano.herokuapp.com/), [Prodigy](https://prodi.gy/) (by Spacy creators, paid), [Labelbox](https://www.labelbox.com/), or [Brat](http://brat.nlplab.org/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b2bab6-e700-4ac7-9362-0b6f14f1534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"John S. Maro lives in New York and is tired. He is 27.2 years old\"\n",
    "entities = [(\"John S. Maro\", \"PERSON\"), (\"New York\", \"LOCATION\"), (\"27.2\",\"AGE\")]\n",
    "\n",
    "# Step 1: Tokenize\n",
    "tokens = text.split()  # Simplistic whitespace tokenization\n",
    "labels = ['O'] * len(tokens)  # Step 2: Initialize with 'O' tags\n",
    "\n",
    "# Step 3: Match entities and assign tags\n",
    "for entity, entity_type in entities:\n",
    "    entity_tokens = entity.split()\n",
    "    for i in range(len(tokens) - len(entity_tokens) + 1):\n",
    "        if tokens[i:i+len(entity_tokens)] == entity_tokens:\n",
    "            labels[i] = \"B-\" + entity_type\n",
    "            for j in range(1, len(entity_tokens)):\n",
    "                labels[i+j] = \"I-\" + entity_type\n",
    "\n",
    "# Step 4: Compile to CoNLL format\n",
    "conll_data = \"\\n\".join([f\"{token} {label}\" for token, label in zip(tokens, labels)])\n",
    "\n",
    "print(conll_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbef0a9-9890-4789-bde3-a02649b8748e",
   "metadata": {},
   "source": [
    "### 3. **Train/Test Split**\n",
    "- Consider an 80% training, 10% validation, and 10% test split.\n",
    "- Respect document boundaries to avoid overlap between sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ff4a8-69a3-49e1-a304-0bb4b0b6a729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03e117e8-49f2-47dc-91d8-b4b8466f1cf0",
   "metadata": {},
   "source": [
    "### 4. **Preprocessing**\n",
    "- Tokenize consistently with the pre-trained model's tokenization.\n",
    "- Other steps might include converting to lowercase, handling punctuation, etc.\n",
    "\n",
    "### 5. **Model-Specific Formatting**\n",
    "- Convert data to be compatible with your chosen framework.\n",
    "- For HuggingFace Transformers, use their `TokenClassification` model format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33293be4-3580-4197-b5d6-3991fe7b584c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "add9b995-d2d0-4c0a-ba8d-d507c7244417",
   "metadata": {},
   "source": [
    "### 6. **Augmentation (Optional)**\n",
    "For smaller datasets, consider:\n",
    "- Back translation\n",
    "- Synonym replacement\n",
    "- Sentence shuffling\n",
    "\n",
    "### 7. **Data Quality Checks**\n",
    "- Ensure annotation consistency.\n",
    "- Address issues like overlapping annotations or mislabeled entities.\n",
    "\n",
    "After data preparation, proceed with fine-tuning your NER model, evaluating on the validation set and tuning hyperparameters as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ebed2-ee91-4d28-9e0e-09279fce6a11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
