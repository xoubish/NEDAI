{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "405ffb67-430e-4d98-8670-a0ec5453358d",
   "metadata": {},
   "source": [
    "# Preparing Data for Fine-tuning a NER Model\n",
    "started Oct 17th, last edit: Oct 31st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f56069-da58-4756-81bf-59f021663f76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a679815f-89fb-49e6-8713-e49e4d929a15",
   "metadata": {},
   "source": [
    "## 1. **Data/Label Collection**\n",
    "- Reading in htmls, extracting text and table from it\n",
    "- Reading in labels extracted by NED figuring out where in the text/table they come from\n",
    "- Add the location in text/table as an extra column to the label file (to be used for annotation).\n",
    "- If labeled dataset is insufficient, we should consider augmenting it with more representative examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ed9ac1-98ab-462e-80b9-26d71ccfbde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(s):\n",
    "    \"\"\"Clean the string by retaining only alphabetical and numerical values and converting to lowercase.\"\"\"\n",
    "    return re.sub(r'[^a-zA-Z0-9]', '', s).lower()\n",
    "\n",
    "def generate_variants(s):\n",
    "    \"\"\"Generate various forms of the string.\"\"\"\n",
    "    s = str(s).strip()\n",
    "    cleaned = clean_string(s)\n",
    "    with_dash = s.replace(' ', '-')\n",
    "    with_mdash = s.replace(' ', '_')\n",
    "    without_mdash = s.replace('-',' ')\n",
    "    withou_dash = s.replace('_', ' ')\n",
    "    nospace = s.replace(' ','')\n",
    "    variants = [s, cleaned, with_dash, with_mdash,nospace,withou_dash,without_mdash]\n",
    "    return variants\n",
    "\n",
    "def cleantable(df):\n",
    "    '''change multi-index column to single and \n",
    "    Iterate through each row and column in the DataFrame to remove non byte like characters'''\n",
    "    \n",
    "    df.columns = [' '.join(col).strip() for col in df.columns.values]\n",
    "    for index, row in df.iterrows():\n",
    "        for col in df.columns:\n",
    "            df.at[index, col] = re.sub('[^a-zA-Z0-9]', '', str(row[col]))\n",
    "    return df\n",
    "\n",
    "def clean_tables_list(tables_list):\n",
    "    '''Iterate over a list of tables and apply cleantable function to each one'''\n",
    "    return [cleantable(df) for df in tables_list]\n",
    "\n",
    "def get_html_tables(f):\n",
    "    with open(f) as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "    try:\n",
    "        tables = pd.read_html(str(soup))\n",
    "        clean_tables_list(tables)\n",
    "        return tables\n",
    "    except:\n",
    "        #print('no table in'+str(f))\n",
    "        pass\n",
    "    \n",
    "def get_html_text(f,plength=100):\n",
    "    with open(f) as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "        \n",
    "    #to inspect html and identify the class label\n",
    "    #print(soup.prettify()) \n",
    "    \n",
    "    sections = soup.find_all('div', class_=\"article-text\")\n",
    "\n",
    "    # Extracting all paragraphs in the section\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text = ''\n",
    "    for i, para in enumerate(paragraphs):\n",
    "        p = para.get_text()\n",
    "        if (len(p)>plength) and (p[0].isalpha()):\n",
    "            text+=p\n",
    "            #print(f\"Paragraph {i+1}:\", p)\n",
    "            #print('--------------')\n",
    "    #text = re.sub(r'[^a-zA-Z0-9 .,]', '', text)#.lower()\n",
    "    return text\n",
    "\n",
    "def find_all(text, substring):\n",
    "    return [match.start() for match in re.finditer(substring, text, re.IGNORECASE)]\n",
    "\n",
    "def find_variant_in_text(sub, text):\n",
    "    \"\"\"Find the start and end index of a variant of 'sub' in 'text'.\"\"\"\n",
    "    ls =[]\n",
    "    for variant in generate_variants(sub):\n",
    "        start_index = find_all(text,variant)\n",
    "        if start_index:  # This condition checks if the list is not empty\n",
    "            for s in start_index:\n",
    "                ls.append((s, s + len(variant)))\n",
    "    return ls if ls else None\n",
    "\n",
    "def find_variant_in_tables(sub, tables_list):\n",
    "    \"\"\"Find all table numbers, row indices, and column names where a variant of 'sub' exists.\"\"\"\n",
    "    locations = []\n",
    "    \n",
    "    for variant in generate_variants(str(sub)):\n",
    "        for table_num, table in enumerate(tables_list, 1):\n",
    "            locs = table.isin([variant]).stack()\n",
    "            matched_locs = locs[locs == True].index.tolist()\n",
    "            for row, col in matched_locs:\n",
    "                locations.append((table_num, row, col))\n",
    "                \n",
    "    return locations if locations else None\n",
    "    \n",
    "def add_location_columns(df, text,tables_list,colname='ap_name1',text_only=True):\n",
    "    \"\"\"Add 'Location' column to DataFrame based on the 'ap_name1' column values' appearance in the text.\"\"\"\n",
    "    if colname in df.columns:\n",
    "        df[colname+'_Loc_in_text'] = df[colname].apply(lambda cell: find_variant_in_text(cell, text) if pd.notna(cell) else None)\n",
    "        if (tables_list) and (text_only==False):\n",
    "            df[colname+'_Loc_in_table'] = df[colname].apply(lambda cell: find_variant_in_tables(cell, tables_list) if pd.notna(cell) else None)\n",
    "        else:\n",
    "            df[colname+'_Loc_in_table'] = None\n",
    "    return df\n",
    "\n",
    "def find_token_number_by_index(tokens, index):\n",
    "    current_index = 0\n",
    "\n",
    "    for token_num, token in enumerate(tokens):\n",
    "        start_index = texts.find(token, current_index)\n",
    "        end_index = start_index + len(token)\n",
    "        \n",
    "        if index >= start_index and index <= end_index:\n",
    "            return token_num#, token\n",
    "        \n",
    "        current_index = end_index  # Set current index to the end of the current token\n",
    "\n",
    "    return None\n",
    "    \n",
    "def find_variant_in_text_pro(sub, text):\n",
    "    \"\"\"Find the sentence number and token index of a variant of 'sub' in 'text', along with the total number of parts in the text.\"\"\"\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    results = set()\n",
    "\n",
    "    for sentence_num, sentence in enumerate(sentences):\n",
    "        tokens = word_tokenize(sentence)\n",
    "        \n",
    "        # Consider pairs of tokens for matching\n",
    "        for token_index in range(len(tokens) - 1):  # -1 because we're looking at pairs\n",
    "            token_pair = tokens[token_index] + ' ' + tokens[token_index + 1]\n",
    "            \n",
    "            for variant in generate_variants(sub):\n",
    "                if token_pair == variant:\n",
    "                    results.add((sentence_num, token_index, 2))\n",
    "\n",
    "                # Also check individual tokens if needed\n",
    "                elif tokens[token_index] == variant:\n",
    "                    results.add((sentence_num, token_index, 1))\n",
    "                    \n",
    "    return list(results) if results else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dda383-ea74-4c51-8328-ebdfc7e26493",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'data/2022ApJ_PREPFILES/'\n",
    "outdir = 'data/lable_locations2/'\n",
    "\n",
    "count = 0\n",
    "for prepfilename in tqdm(os.listdir(directory)):\n",
    "    if count>1000:\n",
    "        break\n",
    "    prepfilepath = os.path.join(directory, prepfilename)\n",
    "    if os.path.isfile(prepfilepath):\n",
    "        \n",
    "        # Read in label file \n",
    "        labeldf = pd.read_csv(prepfilepath, delimiter=\"|\", skipinitialspace=True, low_memory=False)\n",
    "        labeldf.columns = labeldf.columns.str.strip()\n",
    "        \n",
    "\n",
    "        try:\n",
    "            # Read in html sections and tables\n",
    "            s = prepfilename.split('.')\n",
    "            htmldir = 'data/'+s[0][0:4]+'-'+s[0][4:]+'-Vol'+s[3][0:3]+'/HTML/'\n",
    "            htmlfilepath = os.path.join(htmldir, prepfilename[0:19]+'.html')\n",
    "            texts = get_html_text(htmlfilepath)\n",
    "            tables = get_html_tables(htmlfilepath)\n",
    "            #find where in section texts or table the labeled data are mentioned\n",
    "            df = add_location_columns(labeldf,texts,tables,colname='ap_name1',text_only=True)\n",
    "            #df = add_location_columns(labeldf,texts,tables,colname='name1',text_only=True)\n",
    "            df = add_location_columns(labeldf,texts,tables,colname='vz1',text_only=True)\n",
    "            df = add_location_columns(labeldf,texts,tables,colname='coordx1',text_only=True)\n",
    "            df = add_location_columns(labeldf,texts,tables,colname='coordy1',text_only=True)\n",
    "            #df = add_location_columns(labeldf,texts,tables,colname='type1')\n",
    "            \n",
    "            df.to_csv(outdir+prepfilename[:-3]+'csv', index=False) # commented so no overwrite now\n",
    "            count+=1\n",
    "        except Exception as e:\n",
    "            print('---in file:'+prepfilename+'---')\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            print('---------------------------')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320d2c15-b4c9-4fc3-a1af-02f9e831e5ba",
   "metadata": {},
   "source": [
    "## 2. **Annotation**\n",
    "- Mark and label entities within your text.\n",
    "- Entities to start with: `Object Name`, `RA`, `DEC`, `Redshift`, `Type`. We may add more later.\n",
    "#### 2.1 **Figure out annotation formats**\n",
    "Data can be represented in various formats:\n",
    "- **BIO (or IOB) Format** (Begining/Inside/Outside)\n",
    "- **CoNLL Format**: Columns-based, used in datasets like CoNLL-2003. (BIO seems to be under this?)\n",
    "- **Spacy Format**: JSON format (for Spacy users) with entities represented by start/end character positions.\n",
    "\n",
    "Manual annotation can be time-consuming. If NED had not already done some part of this we could have considered: [Doccano](https://doccano.herokuapp.com/), [Prodigy](https://prodi.gy/) (by Spacy creators, paid), [Labelbox](https://www.labelbox.com/), or [Brat](http://brat.nlplab.org/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ebf1d-d24a-4ea1-947f-4439fd868802",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = 'data/lable_locations/'\n",
    "\n",
    "labelcolumns = ['ap_name1_Loc_in_text', 'vz1_Loc_in_text','coordx1_Loc_in_text','coordy1_Loc_in_text','type1_Loc_in_text']\n",
    "valuecolumns = ['ap_name1','vz1','coordx1','coordy1','type1']\n",
    "outpcolumns = ['name','redshift','RA','DEC','Type']\n",
    "\n",
    "count = 0\n",
    "for labelfilename in tqdm(os.listdir(outdir)):\n",
    "\n",
    "    filename = \"data/conlls2/\" + labelfilename[:-4] + \".conll\"\n",
    "    labelfilepath = os.path.join(outdir, labelfilename)\n",
    "    if os.path.isfile(labelfilepath) and not os.path.exists(filename):\n",
    "        try:\n",
    "            # Read in csv and html text \n",
    "            labeldf = pd.read_csv(labelfilepath, encoding=None)\n",
    "            s = labelfilename.split('.')\n",
    "            htmldir = 'data/'+s[0][0:4]+'-'+s[0][4:]+'-Vol'+s[3][0:3]+'/HTML/'\n",
    "            htmlfilepath = os.path.join(htmldir, labelfilename[0:19]+'.html')\n",
    "            texts = get_html_text(htmlfilepath)\n",
    "            #tables = get_html_tables(htmlfilepath)\n",
    "        \n",
    "            # Step 1: Tokenize, Split text into sentences\n",
    "            sentences = sent_tokenize(texts)\n",
    "        \n",
    "            conll_data = \"\"\n",
    "            for sentence_num, sentence in enumerate(sentences):\n",
    "                tokens = word_tokenize(sentence)\n",
    "                labels = ['O'] * len(tokens)  # Initialize with 'O' tags\n",
    "\n",
    "                # Consider pairs of tokens for matching\n",
    "                for token_index in range(len(tokens) - 1):  # -1 because we're looking at pairs\n",
    "                    token_pair = tokens[token_index] + ' ' + tokens[token_index + 1]\n",
    "            \n",
    "                    for val in valuecolumns:\n",
    "                        if val in labeldf.columns:\n",
    "                            for sub in labeldf[val]:\n",
    "                                for variant in generate_variants(sub):\n",
    "                                    if token_pair == variant:\n",
    "                                        labels[token_index] = \"B-\" + val\n",
    "                                        labels[token_index+1] = \"I-\" + val\n",
    "                                    \n",
    "                                    # Also check individual tokens if needed\n",
    "                                    elif tokens[token_index] == variant:\n",
    "                                        labels[token_index] = \"B-\" + val\n",
    "                            # Append to the conll_data\n",
    "                conll_data += \"\\n\".join([f\"{token} {label}\" for token, label in zip(tokens, labels)])\n",
    "                conll_data += \"\\n\\n\"  # Add an extra newline to separate sentences\n",
    "            with open(filename, 'w', encoding='utf-8') as file:\n",
    "                file.write(conll_data)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "raw",
   "id": "afcff1ce-2826-41bd-bab4-ac26818934e5",
   "metadata": {},
   "source": [
    "# old version, issue was tokenization and indexing didn't match for sentence and word\n",
    "    \n",
    "import ast\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "outdir = 'data/lable_locations/'\n",
    "\n",
    "labelcolumns = ['ap_name1_Loc_in_text', 'vz1_Loc_in_text','coordx1_Loc_in_text','coordy1_Loc_in_text','type1_Loc_in_text']\n",
    "valuecolumns = ['ap_name1','vz1','coordx1','coordy1','type1']\n",
    "outpcolumns = ['name','redshift','RA','DEC','Type']\n",
    "\n",
    "count = 0\n",
    "for labelfilename in tqdm(os.listdir(outdir)):\n",
    "    labelfilepath = os.path.join(outdir, labelfilename)\n",
    "    if os.path.isfile(labelfilepath):\n",
    "        # Read in label file \n",
    "        try:\n",
    "            labeldf = pd.read_csv(labelfilepath, encoding=None)\n",
    "            s = labelfilename.split('.')\n",
    "            htmldir = 'data/'+s[0][0:4]+'-'+s[0][4:]+'-Vol'+s[3][0:3]+'/HTML/'\n",
    "            htmlfilepath = os.path.join(htmldir, labelfilename[0:19]+'.html')\n",
    "            texts = get_html_text(htmlfilepath)\n",
    "            #tables = get_html_tables(htmlfilepath)\n",
    "        \n",
    "            # Step 1: Tokenize\n",
    "            # Split text into sentences\n",
    "            sentences = sent_tokenize(texts)\n",
    "\n",
    "            conll_data = \"\"\n",
    "            for sentence in sentences:\n",
    "                # Tokenize the sentence\n",
    "                tokens = word_tokenize(sentence)\n",
    "                labels = ['O'] * len(tokens)  # Initialize with 'O' tags\n",
    "\n",
    "                for val, lab, outval in zip(valuecolumns, labelcolumns, outpcolumns):\n",
    "                    if lab in labeldf.columns:\n",
    "                        for item1, item2 in zip(labeldf[val], labeldf[lab]):\n",
    "                            if pd.notna(item2):                        \n",
    "                                # Convert the string to an actual list of tuples\n",
    "                                firstindices = np.array(ast.literal_eval(item2))[:,0]\n",
    "                                lastindices = np.array(ast.literal_eval(item2))[:,1]\n",
    "\n",
    "                                for i, j in zip(firstindices, lastindices):\n",
    "                                    token_i = find_token_number_by_index(tokens, i)\n",
    "                                    token_j = find_token_number_by_index(tokens, j)\n",
    "                                    labels[token_i] = \"B-\" + outval\n",
    "                                    if token_j != token_i:\n",
    "                                        for t in range(token_i + 1, token_j + 1):\n",
    "                                            labels[t] = \"I-\" + outval\n",
    "\n",
    "                        # Append to the conll_data\n",
    "                conll_data += \"\\n\".join([f\"{token} {label}\" for token, label in zip(tokens, labels)])\n",
    "                conll_data += \"\\n\\n\"  # Add an extra newline to separate sentences\n",
    "\n",
    "            filename = \"data/conlls/\" + labelfilename[:-4] + \".conll\"\n",
    "            with open(filename, 'w', encoding='utf-8') as file:\n",
    "                file.write(conll_data)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbef0a9-9890-4789-bde3-a02649b8748e",
   "metadata": {},
   "source": [
    "### 3. **Train/Test Split**\n",
    "- Consider an 80% training, 10% validation, and 10% test split.\n",
    "- Respect document boundaries to avoid overlap between sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ff4a8-69a3-49e1-a304-0bb4b0b6a729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path to your folder containing the .txt files\n",
    "folder_path = 'data/conlls2/'\n",
    "\n",
    "# Read all the files and combine their content\n",
    "all_data = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.conll'):\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as f:\n",
    "            all_data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32320724-34f3-4f62-9ae0-e6b708f2f5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split 80% for training, 10% for validation, and 10% for testing\n",
    "train_data, temp_data = train_test_split(all_data, test_size=0.2, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5369dbf-af79-4ef7-8e00-6a9928616bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"\\n\\n\".join(train_data))\n",
    "\n",
    "with open('val.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"\\n\\n\".join(val_data))\n",
    "\n",
    "with open('test.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"\\n\\n\".join(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ab8cac-d8fb-4a15-96b3-211c45943efe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"data/train.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "filtered_lines = []\n",
    "current_sentence = []\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line:  # If the line is not empty\n",
    "        current_sentence.append(line)\n",
    "        continue\n",
    "    else:  # If the line is empty (end of a sentence)\n",
    "        if any(tag.startswith((\"B-\", \"I-\")) for _, tag in (l.split() for l in current_sentence)):\n",
    "            filtered_lines.extend(current_sentence)\n",
    "            filtered_lines.append(\"\")  # Empty line to denote end of sentence\n",
    "        current_sentence = []\n",
    "\n",
    "# Handle the last sentence if it didn't end with an empty line\n",
    "if current_sentence and any(tag.startswith((\"B-\", \"I-\")) for _, tag in (l.split() for l in current_sentence)):\n",
    "    filtered_lines.extend(current_sentence)\n",
    "\n",
    "# Write the filtered lines back to the file or to a new file\n",
    "with open(\"filtered_train.txt\", \"w\") as file:\n",
    "    for line in filtered_lines:\n",
    "        file.write(line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7d8b52-7986-469e-9602-40b8fe4f8ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"data/test.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "filtered_lines = []\n",
    "current_sentence = []\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line:  # If the line is not empty\n",
    "        current_sentence.append(line)\n",
    "        continue\n",
    "    else:  # If the line is empty (end of a sentence)\n",
    "        if any(tag.startswith((\"B-\", \"I-\")) for _, tag in (l.split() for l in current_sentence)):\n",
    "            filtered_lines.extend(current_sentence)\n",
    "            filtered_lines.append(\"\")  # Empty line to denote end of sentence\n",
    "        current_sentence = []\n",
    "\n",
    "# Handle the last sentence if it didn't end with an empty line\n",
    "if current_sentence and any(tag.startswith((\"B-a\", \"I-a\",\"B-v\",\"I-v\")) for _, tag in (l.split() for l in current_sentence)):\n",
    "    filtered_lines.extend(current_sentence)\n",
    "\n",
    "# Write the filtered lines back to the file or to a new file\n",
    "with open(\"filtered_test.txt\", \"w\") as file:\n",
    "    for line in filtered_lines:\n",
    "        file.write(line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d4cb4e-6771-4f4a-b1be-702aac5c77bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"data/val.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "filtered_lines = []\n",
    "current_sentence = []\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line:  # If the line is not empty\n",
    "        current_sentence.append(line)\n",
    "        continue\n",
    "    else:  # If the line is empty (end of a sentence)\n",
    "        if any(tag.startswith((\"B-\", \"I-\")) for _, tag in (l.split() for l in current_sentence)):\n",
    "            filtered_lines.extend(current_sentence)\n",
    "            filtered_lines.append(\"\")  # Empty line to denote end of sentence\n",
    "        current_sentence = []\n",
    "\n",
    "# Handle the last sentence if it didn't end with an empty line\n",
    "if current_sentence and any(tag.startswith((\"B-a\", \"I-a\",\"B-v\",\"I-v\")) for _, tag in (l.split() for l in current_sentence)):\n",
    "    filtered_lines.extend(current_sentence)\n",
    "\n",
    "# Write the filtered lines back to the file or to a new file\n",
    "with open(\"filtered_val.txt\", \"w\") as file:\n",
    "    for line in filtered_lines:\n",
    "        file.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e117e8-49f2-47dc-91d8-b4b8466f1cf0",
   "metadata": {},
   "source": [
    "### 4. **Preprocessing**\n",
    "- Tokenize consistently with the pre-trained model's tokenization.\n",
    "- Other steps might include converting to lowercase, handling punctuation, etc.\n",
    "\n",
    "### 5. **Model-Specific Formatting**\n",
    "- Convert data to be compatible with your chosen framework.\n",
    "- For HuggingFace Transformers, use their `TokenClassification` model format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33293be4-3580-4197-b5d6-3991fe7b584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    'train': 'train.txt',\n",
    "    'validation': 'val.txt',\n",
    "    'test': 'test.txt'\n",
    "}\n",
    "\n",
    "# Load the dataset from local files without specifying a script\n",
    "dataset = load_dataset('text', data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da6ac99-b502-41d9-8bf0-8f64a47afd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(batch):\n",
    "    # Process each line in the batch\n",
    "    tokens_list = []\n",
    "    tags_list = []\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    \n",
    "    for line in batch['text']:\n",
    "        if line:  # non-empty line means we have a token-tag pair\n",
    "            token, tag = line.split()  # assuming space is the delimiter\n",
    "            tokens.append(token)\n",
    "            tags.append(tag)\n",
    "        else:  # empty line means end of sentence\n",
    "            tokens_list.append(tokens)\n",
    "            tags_list.append(tags)\n",
    "            tokens = []\n",
    "            tags = []\n",
    "    \n",
    "    # Add remaining tokens and tags if there's any\n",
    "    if tokens:\n",
    "        tokens_list.append(tokens)\n",
    "        tags_list.append(tags)\n",
    "    \n",
    "    return {'tokens': tokens_list, 'tags': tags_list}\n",
    "\n",
    "# Apply the processing function on all splits of the dataset\n",
    "pdataset = dataset.map(process_sample, batched=True, remove_columns=['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae1263-a314-42ce-9ea1-b88cf932c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9049632b-5a75-44fa-b5d5-4159205f99ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add9b995-d2d0-4c0a-ba8d-d507c7244417",
   "metadata": {},
   "source": [
    "### 6. **Augmentation (Optional)**\n",
    "For smaller datasets, consider:\n",
    "- Back translation\n",
    "- Synonym replacement\n",
    "- Sentence shuffling\n",
    "\n",
    "### 7. **Data Quality Checks**\n",
    "- Ensure annotation consistency.\n",
    "- Address issues like overlapping annotations or mislabeled entities.\n",
    "\n",
    "After data preparation, proceed with fine-tuning your NER model, evaluating on the validation set and tuning hyperparameters as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cee01be-5263-499e-adf1-777f074babf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def count_rows_in_column(folder_path: str, column_name: str) -> int:\n",
    "    \"\"\"\n",
    "    Counts non-empty rows in a specified column across all CSV files in a given folder.\n",
    "    \n",
    "    Args:\n",
    "    - folder_path (str): Path to the folder containing CSV files.\n",
    "    - column_name (str): Name of the column to count non-empty rows for.\n",
    "\n",
    "    Returns:\n",
    "    - int: Total count of non-empty rows for the specified column across all CSV files.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_count = 0\n",
    "    \n",
    "    # Loop over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file is a CSV\n",
    "        if filename.endswith('.csv'):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                # Read the CSV file into a DataFrame\n",
    "                df = pd.read_csv(filepath)\n",
    "                \n",
    "                # Check if the column exists\n",
    "                if column_name in df.columns:\n",
    "                    # Count non-empty rows (assuming NaN as empty)\n",
    "                    count = df[column_name].count()\n",
    "                    total_count += count\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "                \n",
    "    return total_count\n",
    "\n",
    "labdir = 'data/lable_locations/'\n",
    "# Columns you are interested in\n",
    "coln = ['ap_name1', 'ap_name1_Loc_in_text','ap_name1_Loc_in_table']\n",
    "colz = ['vz1', 'vz1_Loc_in_text','vz1_Loc_in_table']\n",
    "colx = ['coordx1', 'coordx1_Loc_in_text','coordx1_Loc_in_table']\n",
    "\n",
    "ncounts,zcounts,xcounts = [],[],[]\n",
    "for c in range(len(coln)):\n",
    "    ncounts.append(count_rows_in_column(labdir,coln[c]))\n",
    "    xcounts.append(count_rows_in_column(labdir,colx[c]))\n",
    "    zcounts.append(count_rows_in_column(labdir,colz[c]))\n",
    "    \n",
    "# Bar width and positions\n",
    "barWidth = 0.25\n",
    "r1 = np.arange(len(ncounts))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "\n",
    "# Create bars\n",
    "plt.bar(r1, ncounts, width=barWidth, edgecolor='grey', label='Names')\n",
    "plt.bar(r2, xcounts, width=barWidth, edgecolor='grey', label='coords')\n",
    "plt.bar(r3, zcounts, width=barWidth, edgecolor='grey', label='redshift')\n",
    "labels = ['in prep file','found in text','found in tables']\n",
    "# Title & subtitle\n",
    "plt.xticks([r + barWidth for r in range(len(ncounts))], labels)\n",
    "plt.legend()\n",
    "plt.ylabel('number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d479425-51cb-42b0-acf1-71c2f60160e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
