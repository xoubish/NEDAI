{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "405ffb67-430e-4d98-8670-a0ec5453358d",
   "metadata": {},
   "source": [
    "# Preparing Data for Fine-tuning a NER Model\n",
    "started Oct 17th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f56069-da58-4756-81bf-59f021663f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a679815f-89fb-49e6-8713-e49e4d929a15",
   "metadata": {},
   "source": [
    "### 1. **Data/Label Collection**\n",
    "- Reading in htmls, extracting text and table from it\n",
    "- Reading in labels extracted by NED figuring out where in the text/table they come from\n",
    "- Add the location in text/table as an extra column to the label file (to be used for annotation).\n",
    "- If labeled dataset is insufficient, we should consider augmenting it with more representative examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03ed9ac1-98ab-462e-80b9-26d71ccfbde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(s):\n",
    "    \"\"\"Clean the string by retaining only alphabetical and numerical values and converting to lowercase.\"\"\"\n",
    "    return re.sub(r'[^a-zA-Z0-9]', '', s).lower()\n",
    "\n",
    "def generate_variants(s):\n",
    "    \"\"\"Generate various forms of the string.\"\"\"\n",
    "    s = str(s).strip()\n",
    "    cleaned = clean_string(s)\n",
    "    with_dash = s.replace(' ', '-')\n",
    "    with_mdash = s.replace(' ', '—')\n",
    "    variants = [s, cleaned]#, with_dash, with_mdash]\n",
    "    return variants\n",
    "\n",
    "def cleantable(df):\n",
    "    '''change multi-index column to single and \n",
    "    Iterate through each row and column in the DataFrame to remove non byte like characters'''\n",
    "    \n",
    "    df.columns = [' '.join(col).strip() for col in df.columns.values]\n",
    "    for index, row in df.iterrows():\n",
    "        for col in df.columns:\n",
    "            df.at[index, col] = re.sub('[^a-zA-Z0-9]', '', str(row[col]))\n",
    "    return df\n",
    "\n",
    "def clean_tables_list(tables_list):\n",
    "    '''Iterate over a list of tables and apply cleantable function to each one'''\n",
    "    return [cleantable(df) for df in tables_list]\n",
    "\n",
    "def get_html_tables(f):\n",
    "    with open(f) as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "    try:\n",
    "        tables = pd.read_html(str(soup))\n",
    "        clean_tables_list(tables)\n",
    "        return tables\n",
    "    except:\n",
    "        #print('no table in'+str(f))\n",
    "        pass\n",
    "    \n",
    "def get_html_text(f,plength=100):\n",
    "    with open(f) as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "        \n",
    "    #to inspect html and identify the class label\n",
    "    #print(soup.prettify()) \n",
    "    \n",
    "    sections = soup.find_all('div', class_=\"article-text\")\n",
    "\n",
    "    # Extracting all paragraphs in the section\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text = ''\n",
    "    for i, para in enumerate(paragraphs):\n",
    "        p = para.get_text()\n",
    "        if (len(p)>plength) and (p[0].isalpha()):\n",
    "            text+=p\n",
    "            #print(f\"Paragraph {i+1}:\", p)\n",
    "            #print('--------------')\n",
    "    return text\n",
    "\n",
    "def find_all(text, substring):\n",
    "    return [match.start() for match in re.finditer(substring, text, re.IGNORECASE)]\n",
    "\n",
    "def find_variant_in_text(sub, text):\n",
    "    \"\"\"Find the start and end index of a variant of 'sub' in 'text'.\"\"\"\n",
    "    ls =[]\n",
    "    for variant in generate_variants(sub):\n",
    "        start_index = find_all(text,variant)\n",
    "        if start_index:  # This condition checks if the list is not empty\n",
    "            for s in start_index:\n",
    "                ls.append((s, s + len(variant)))\n",
    "    return ls if ls else None\n",
    "\n",
    "def find_variant_in_tables(sub, tables_list):\n",
    "    \"\"\"Find all table numbers, row indices, and column names where a variant of 'sub' exists.\"\"\"\n",
    "    locations = []\n",
    "    \n",
    "    for variant in generate_variants(str(sub)):\n",
    "        for table_num, table in enumerate(tables_list, 1):\n",
    "            locs = table.isin([variant]).stack()\n",
    "            matched_locs = locs[locs == True].index.tolist()\n",
    "            for row, col in matched_locs:\n",
    "                locations.append((table_num, row, col))\n",
    "                \n",
    "    return locations if locations else None\n",
    "    \n",
    "def add_location_columns(df, text,tables_list,colname='ap_name1'):\n",
    "    \"\"\"Add 'Location' column to DataFrame based on the 'ap_name1' column values' appearance in the text.\"\"\"\n",
    "\n",
    "    #df[colname+'_Loc_in_text'] = df[colname].apply(lambda cell: find_variant_in_text(cell, text) if pd.notna(cell) else None)\n",
    "    df[colname+'_Loc_in_text'] = df[colname].apply(lambda cell: find_variant_in_text(cell, text) if pd.notna(cell) else None)\n",
    "\n",
    "    if tables_list:\n",
    "        df[colname+'_Loc_in_table'] = df[colname].apply(lambda cell: find_variant_in_tables(cell, tables_list) if pd.notna(cell) else None)\n",
    "    else:\n",
    "        df[colname+'_Loc_in_table'] = None\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1dda383-ea74-4c51-8328-ebdfc7e26493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████▎                                     | 115/930 [01:02<06:35,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---in file:2022ApJ...941...18Z_A18Z.flt---\n",
      "An error occurred: 'ap_name1'\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████▏                                    | 135/930 [01:13<05:00,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---in file:2022ApJ...925L..20K_L20K.flt---\n",
      "An error occurred: [Errno 2] No such file or directory: 'data/2022-ApJ-Vol925/HTML/2022ApJ...925L..20K.html'\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|█████████▍                                 | 203/930 [01:53<06:47,  1.78it/s]\n"
     ]
    }
   ],
   "source": [
    "directory = 'data/2022ApJ_PREPFILES/'\n",
    "outdir = 'data/lable_locations/'\n",
    "\n",
    "count = 0\n",
    "for prepfilename in tqdm(os.listdir(directory)):\n",
    "    if count>200:\n",
    "        break\n",
    "    prepfilepath = os.path.join(directory, prepfilename)\n",
    "    if os.path.isfile(prepfilepath):\n",
    "        \n",
    "        # Read in label file \n",
    "        labeldf = pd.read_csv(prepfilepath, delimiter=\"|\", skipinitialspace=True, low_memory=False)\n",
    "        labeldf.columns = labeldf.columns.str.strip()\n",
    "        \n",
    "\n",
    "        try:\n",
    "            # Read in html sections and tables\n",
    "            s = prepfilename.split('.')\n",
    "            htmldir = 'data/'+s[0][0:4]+'-'+s[0][4:]+'-Vol'+s[3][0:3]+'/HTML/'\n",
    "            htmlfilepath = os.path.join(htmldir, prepfilename[0:19]+'.html')\n",
    "            texts = get_html_text(htmlfilepath)\n",
    "            tables = get_html_tables(htmlfilepath)\n",
    "            \n",
    "            #find where in section texts or table the labeled data are mentioned\n",
    "            df = add_location_columns(labeldf,texts,tables,colname='ap_name1')\n",
    "            df.to_csv(outdir+prepfilename[:-3]+'csv', index=False)\n",
    "            count+=1\n",
    "        except Exception as e:\n",
    "            print('---in file:'+prepfilename+'---')\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            print('---------------------------')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f555dd6-63d2-4c17-92ed-c3149e2975d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Read in label file \n",
    "labeldf = pd.read_csv('data/2022ApJ_PREPFILES/2022ApJ...926..167J_A167J.flt', delimiter=\"|\", skipinitialspace=True)\n",
    "labeldf.columns = labeldf.columns.str.strip()\n",
    "        \n",
    "htmlfilepath = 'data/2022-ApJ-Vol926/HTML/2022ApJ...926..167J.html'\n",
    "texts = get_html_text(htmlfilepath)\n",
    "tables = get_html_tables(htmlfilepath)\n",
    "\n",
    "#find where in section texts or table the labeled data are mentioned\n",
    "df = add_location_columns(labeldf,texts,tables,colname='ap_name1')\n",
    "df.to_csv('data/lable_locations/test.csv', index=False)\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320d2c15-b4c9-4fc3-a1af-02f9e831e5ba",
   "metadata": {},
   "source": [
    "### 2. **Annotation**\n",
    "- Mark and label entities within your text.\n",
    "- Entities to start with: `Object Name`, `RA`, `DEC`, `Redshift`, `Type`. We may add more later.\n",
    "#### 2.1 **Figure out annotation formats**\n",
    "Data can be represented in various formats:\n",
    "- **BIO (or IOB) Format**\n",
    "- **CoNLL Format**: Columns-based, used in datasets like CoNLL-2003. **will go with this for now**\n",
    "- **Spacy Format**: JSON format (for Spacy users) with entities represented by start/end character positions.\n",
    "\n",
    "Manual annotation can be time-consuming. If NED had not already done some part of this we could have considered: [Doccano](https://doccano.herokuapp.com/), [Prodigy](https://prodi.gy/) (by Spacy creators, paid), [Labelbox](https://www.labelbox.com/), or [Brat](http://brat.nlplab.org/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b2bab6-e700-4ac7-9362-0b6f14f1534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"John S. Maro lives in New York and is tired. He is 27.2 years old\"\n",
    "entities = [(\"John S. Maro\", \"PERSON\"), (\"New York\", \"LOCATION\"), (\"27.2\",\"AGE\")]\n",
    "\n",
    "# Step 1: Tokenize\n",
    "tokens = text.split()  # Simplistic whitespace tokenization\n",
    "labels = ['O'] * len(tokens)  # Step 2: Initialize with 'O' tags\n",
    "\n",
    "# Step 3: Match entities and assign tags\n",
    "for entity, entity_type in entities:\n",
    "    entity_tokens = entity.split()\n",
    "    for i in range(len(tokens) - len(entity_tokens) + 1):\n",
    "        if tokens[i:i+len(entity_tokens)] == entity_tokens:\n",
    "            labels[i] = \"B-\" + entity_type\n",
    "            for j in range(1, len(entity_tokens)):\n",
    "                labels[i+j] = \"I-\" + entity_type\n",
    "\n",
    "# Step 4: Compile to CoNLL format\n",
    "conll_data = \"\\n\".join([f\"{token} {label}\" for token, label in zip(tokens, labels)])\n",
    "\n",
    "print(conll_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbef0a9-9890-4789-bde3-a02649b8748e",
   "metadata": {},
   "source": [
    "### 3. **Train/Test Split**\n",
    "- Consider an 80% training, 10% validation, and 10% test split.\n",
    "- Respect document boundaries to avoid overlap between sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ff4a8-69a3-49e1-a304-0bb4b0b6a729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03e117e8-49f2-47dc-91d8-b4b8466f1cf0",
   "metadata": {},
   "source": [
    "### 4. **Preprocessing**\n",
    "- Tokenize consistently with the pre-trained model's tokenization.\n",
    "- Other steps might include converting to lowercase, handling punctuation, etc.\n",
    "\n",
    "### 5. **Model-Specific Formatting**\n",
    "- Convert data to be compatible with your chosen framework.\n",
    "- For HuggingFace Transformers, use their `TokenClassification` model format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33293be4-3580-4197-b5d6-3991fe7b584c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "add9b995-d2d0-4c0a-ba8d-d507c7244417",
   "metadata": {},
   "source": [
    "### 6. **Augmentation (Optional)**\n",
    "For smaller datasets, consider:\n",
    "- Back translation\n",
    "- Synonym replacement\n",
    "- Sentence shuffling\n",
    "\n",
    "### 7. **Data Quality Checks**\n",
    "- Ensure annotation consistency.\n",
    "- Address issues like overlapping annotations or mislabeled entities.\n",
    "\n",
    "After data preparation, proceed with fine-tuning your NER model, evaluating on the validation set and tuning hyperparameters as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ebed2-ee91-4d28-9e0e-09279fce6a11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
